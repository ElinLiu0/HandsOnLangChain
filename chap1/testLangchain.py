from langchain.chat_models import ChatOpenAI

from langchain.schema import HumanMessage

llm = ChatOpenAI(
    streaming=True,
    verbose=True,
    # callbacks=[callback],
    openai_api_key="none",
    openai_api_base="http://127.0.0.1:8000/v1",
    model_name="Qwen/Qwen1.5-4B-Chat"
)
instructions = """
ä½ å°†å¾—åˆ°ä¸€ä¸ªå¸¦æœ‰æ°´æœåç§°çš„å¥å­,æå–è¿™äº›æ°´æœåç§°å¹¶ä¸ºå…¶åˆ†é…ä¸€ä¸ªè¡¨æƒ…ç¬¦å·
åœ¨ python å­—å…¸ä¸­è¿”å›æ°´æœåç§°å’Œè¡¨æƒ…ç¬¦å·
"""

fruit_names = """
è‹¹æœ,æ¢¨,è¿™æ˜¯å¥‡å¼‚æœ
"""

# åˆ¶ä½œç»“åˆè¯´æ˜å’Œæ°´æœåç§°çš„æç¤º
prompt = (instructions + fruit_names)

# Call the LLM
output = llm([HumanMessage(content=prompt)])

print (output.content)
print("========")
print (type(output.content))
# è‹¹æœ: ğŸæ¢¨: ğŸ¥å¥‡å¼‚æœ: ğŸ“
# ========
# <class 'str'>
